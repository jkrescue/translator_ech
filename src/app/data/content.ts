export interface Paragraph {
  id: number;
  type: 'title' | 'authors' | 'affiliation' | 'abstract-label' | 'abstract' | 'section' | 'body' | 'keywords-label' | 'keywords';
  original: string;
  translation: string;
}

export const articleContent: Paragraph[] = [
  {
    id: 1,
    type: 'title',
    original: 'Large Language Models: Transforming Natural Language Processing',
    translation: '大型语言模型：变革自然语言处理',
  },
  {
    id: 2,
    type: 'authors',
    original: 'Andrew Chen¹, Sarah Martinez², Michael Zhang¹, Jennifer Lee³',
    translation: '陈安德鲁¹，萨拉·马丁内斯²，张迈克尔¹，詹妮弗·李³',
  },
  {
    id: 3,
    type: 'affiliation',
    original: '¹Institute for Advanced AI Research, Stanford University  ²MIT Computer Science & AI Lab  ³Carnegie Mellon University',
    translation: '¹斯坦福大学高级人工智能研究院  ²麻省理工学院计算机科学与人工智能实验室  ³卡内基梅隆大学',
  },
  {
    id: 4,
    type: 'keywords-label',
    original: 'Keywords:',
    translation: '关键词：',
  },
  {
    id: 5,
    type: 'keywords',
    original: 'large language models, transformer architecture, natural language processing, reinforcement learning from human feedback, emergent capabilities',
    translation: '大型语言模型，Transformer架构，自然语言处理，基于人类反馈的强化学习，涌现能力',
  },
  {
    id: 6,
    type: 'abstract-label',
    original: 'Abstract',
    translation: '摘要',
  },
  {
    id: 7,
    type: 'abstract',
    original: 'Large language models (LLMs) have fundamentally transformed the landscape of natural language processing (NLP). These models, trained on vast corpora of text data, demonstrate remarkable capabilities across a diverse range of language tasks, including text generation, translation, summarization, and question answering. This paper provides a comprehensive overview of the architecture, training methodology, and applications of modern large language models, with a particular focus on transformer-based architectures.',
    translation: '大型语言模型（LLM）从根本上改变了自然语言处理（NLP）的格局。这些模型在海量文本语料库上进行训练，在文本生成、翻译、摘要和问答等各类语言任务中展现出卓越的能力。本文全面概述了现代大型语言模型的架构、训练方法论及其应用，尤其聚焦于基于Transformer的架构体系。',
  },
  {
    id: 8,
    type: 'abstract',
    original: 'We examine the key innovations that have enabled the development of increasingly capable models, from the original transformer architecture to contemporary systems. Our analysis reveals that scale, data quality, and training techniques are the primary factors driving performance improvements. We also discuss the significant challenges posed by these systems, including computational costs, potential biases, and safety considerations.',
    translation: '我们考察了推动模型能力不断提升的关键创新——从原始Transformer架构到当代系统的演进历程。分析表明，规模、数据质量和训练技术是推动性能提升的主要因素。我们还讨论了这些系统带来的重大挑战，包括计算成本、潜在偏见和安全方面的考量。',
  },
  {
    id: 9,
    type: 'section',
    original: '1.  Introduction',
    translation: '第一节　引言',
  },
  {
    id: 10,
    type: 'body',
    original: 'The field of natural language processing has undergone a remarkable transformation over the past decade. Traditional approaches relied on handcrafted rules, statistical methods, and task-specific architectures. The introduction of neural network-based models, and particularly transformer architectures, has enabled a new paradigm in which a single large model can be adapted to a wide variety of language tasks with minimal task-specific training.',
    translation: '过去十年间，自然语言处理领域经历了深刻变革。传统方法依赖手工规则、统计方法和特定任务架构。神经网络模型、尤其是Transformer架构的引入，开创了全新范式——单一大型模型可在极少特定任务训练的情况下适配各类语言任务。',
  },
  {
    id: 11,
    type: 'body',
    original: 'The development of large language models represents one of the most significant advances in artificial intelligence research. These models are characterized by their massive scale, with the largest systems containing hundreds of billions of parameters trained on trillions of tokens of text. Despite their size, these models have demonstrated an emergent ability to perform tasks they were not explicitly trained for, a phenomenon known as "in-context learning" or "few-shot learning."',
    translation: '大型语言模型的发展是人工智能研究中最重要的进展之一。这些模型以其庞大规模为特征，最大的系统包含数千亿参数，并在数万亿个文本词元上进行训练。尽管规模庞大，这些模型展现出执行未经明确训练任务的涌现能力，这种现象被称为"情境学习"或"小样本学习"。',
  },
  {
    id: 12,
    type: 'body',
    original: 'Recent empirical studies have demonstrated that performance on a wide range of NLP benchmarks scales predictably with model size, dataset size, and compute budget. These scaling laws have guided research and investment decisions, leading to a rapid increase in the scale of models being developed. Understanding these scaling relationships is crucial for predicting the capabilities of future models and allocating research resources effectively.',
    translation: '近期的实证研究表明，在广泛的自然语言处理基准测试中，模型性能随模型规模、数据集大小和计算预算呈可预测的规律性增长。这些缩放定律指导了研究和投资决策，推动了模型规模的快速扩大。理解这些缩放关系对于预测未来模型的能力和有效分配研究资源至关重要。',
  },
  {
    id: 13,
    type: 'section',
    original: '2.  Architecture',
    translation: '第二节　架构',
  },
  {
    id: 14,
    type: 'body',
    original: 'Modern large language models are predominantly based on the transformer architecture, introduced by Vaswani et al. in 2017. The core component of this architecture is the self-attention mechanism, which allows the model to weigh the importance of different tokens in a sequence when making predictions. This mechanism enables the model to capture long-range dependencies in text without the limitations of recurrent neural networks.',
    translation: '现代大型语言模型主要基于Vaswani等人于2017年提出的Transformer架构。该架构的核心组件是自注意力机制，它使模型能够在做出预测时对序列中不同词元的重要性进行权衡。这一机制使模型能够捕捉文本中的长程依赖关系，突破了循环神经网络的局限性。',
  },
  {
    id: 15,
    type: 'body',
    original: 'The architecture typically consists of a stack of transformer blocks, each containing a multi-head self-attention layer and a feed-forward neural network. Layer normalization is applied before each sublayer, and residual connections allow gradients to flow directly through the network during training. The input to the model is a sequence of tokens obtained by tokenizing the input text using a learned vocabulary.',
    translation: '该架构通常由堆叠的Transformer块组成，每个块包含一个多头自注意力层和一个前馈神经网络。每个子层前均应用层归一化，残差连接允许梯度在训练期间直接流经网络。模型的输入是使用学习到的词汇表对输入文本进行分词后得到的词元序列。',
  },
  {
    id: 16,
    type: 'section',
    original: '3.  Training Methodology',
    translation: '第三节　训练方法论',
  },
  {
    id: 17,
    type: 'body',
    original: 'Training large language models requires sophisticated optimization techniques and infrastructure. The primary training objective is typically next-token prediction, where the model learns to predict the probability distribution over the vocabulary for the next token given all previous tokens. This self-supervised approach allows models to be trained on vast amounts of unlabeled text data.',
    translation: '训练大型语言模型需要复杂的优化技术和基础设施。主要训练目标通常是下一个词元预测，即模型学习在给定所有前驱词元的条件下，预测词汇表中下一个词元的概率分布。这种自监督方法使模型能够在大量无标注文本数据上进行训练。',
  },
  {
    id: 18,
    type: 'body',
    original: 'Recent advances have incorporated reinforcement learning from human feedback (RLHF) as a crucial post-training step. This technique involves training a reward model based on human preferences, then using this reward signal to fine-tune the language model using proximal policy optimization or similar algorithms. RLHF has proven essential for aligning model behavior with human values and improving the safety and helpfulness of model outputs.',
    translation: '近期的进展将基于人类反馈的强化学习（RLHF）纳入关键的后训练步骤。该技术通过基于人类偏好训练奖励模型，再利用此奖励信号通过近端策略优化等算法对语言模型进行微调。RLHF已被证明对于将模型行为与人类价值观对齐、提升模型输出的安全性和有用性至关重要。',
  },
  {
    id: 19,
    type: 'section',
    original: '4.  Applications',
    translation: '第四节　应用',
  },
  {
    id: 20,
    type: 'body',
    original: 'Large language models have demonstrated exceptional versatility across numerous domains. In professional settings, these models assist with document drafting, code generation, data analysis, and decision support. In educational contexts, they serve as interactive tutors, providing personalized explanations and feedback to students across all levels of learning.',
    translation: '大型语言模型在众多领域展现出卓越的多功能性。在专业环境中，这些模型协助进行文档起草、代码生成、数据分析和决策支持。在教育场景中，它们充当互动导师，为各学习阶段的学生提供个性化的讲解和反馈。',
  },
  {
    id: 21,
    type: 'body',
    original: 'The medical and scientific communities have also benefited significantly from advances in large language model capabilities. These systems can assist researchers in literature review, hypothesis generation, and experimental design. In clinical settings, language models can support healthcare professionals by summarizing patient records, suggesting differential diagnoses, and providing medication information.',
    translation: '医疗和科学界也从大型语言模型能力的进步中受益颇丰。这些系统能够协助研究人员进行文献综述、假设生成和实验设计。在临床环境中，语言模型可通过整理患者病历、建议鉴别诊断和提供用药信息来支持医疗专业人员的工作。',
  },
  {
    id: 22,
    type: 'section',
    original: '5.  Challenges and Future Directions',
    translation: '第五节　挑战与未来方向',
  },
  {
    id: 23,
    type: 'body',
    original: 'Despite their impressive capabilities, large language models present several significant challenges. The computational cost of training and deploying these models is substantial, raising concerns about environmental impact and accessibility. Models trained on internet-scale data may also reflect and amplify harmful biases present in the training data, leading to outputs that can be unfair or harmful to certain groups.',
    translation: '尽管能力出众，大型语言模型仍面临若干重大挑战。训练和部署这些模型的计算成本巨大，引发了对环境影响和可及性的担忧。在互联网规模数据上训练的模型还可能反映并放大训练数据中存在的有害偏见，导致对某些群体不公平或有害的输出。',
  },
  {
    id: 24,
    type: 'body',
    original: 'Hallucination remains a critical open problem in the field. Language models can generate confident-sounding but factually incorrect statements, which poses serious risks in high-stakes applications such as medical diagnosis or legal advice. Addressing this challenge requires advances in model architecture, training methodology, and inference techniques. Future work will likely focus on improving factual grounding, developing more efficient training methods, and creating better evaluation frameworks.',
    translation: '幻觉现象仍是该领域的关键未解难题。语言模型可能生成听起来自信却与事实不符的陈述，这在医疗诊断或法律建议等高风险应用场景中构成严重风险。应对这一挑战需要在模型架构、训练方法论和推理技术方面取得进展。未来的工作可能将聚焦于提升事实基础、开发更高效的训练方法，以及构建更完善的评估框架。',
  },
  {
    id: 25,
    type: 'section',
    original: 'Conclusion',
    translation: '结论',
  },
  {
    id: 26,
    type: 'body',
    original: 'Large language models have emerged as a transformative technology with profound implications for natural language processing and artificial intelligence more broadly. The rapid pace of development in this field, driven by scaling laws and innovations in training methodology, suggests that we are still in the early stages of understanding the full potential of these systems.',
    translation: '大型语言模型已成为一项变革性技术，对自然语言处理乃至更广泛的人工智能领域产生了深远影响。这一领域在缩放定律和训练方法论创新的推动下发展迅猛，表明我们仍处于充分理解这些系统潜力的早期阶段。',
  },
  {
    id: 27,
    type: 'body',
    original: 'As these models become increasingly integrated into critical systems and everyday life, it is essential that the research community prioritizes not only capability improvements but also safety, fairness, and interpretability. The challenges of bias, hallucination, and computational cost must be addressed systematically to ensure that the benefits of large language models are realized equitably and responsibly across society.',
    translation: '随着这些模型日益融入关键系统和日常生活，研究界不仅要优先提升能力，还必须重视安全性、公平性和可解释性。必须系统性地应对偏见、幻觉和计算成本等挑战，确保大型语言模型的效益得到公平、负责任的社会化实现。',
  },
];

export interface WordEntry {
  phonetic: string;
  partOfSpeech: string;
  translation: string;
  example?: string;
}

export const wordDictionary: Record<string, WordEntry> = {
  'large': { phonetic: '/lɑːrdʒ/', partOfSpeech: 'adj.', translation: '大的，大规模的', example: 'large language models' },
  'language': { phonetic: '/ˈlæŋɡwɪdʒ/', partOfSpeech: 'n.', translation: '语言', example: 'natural language processing' },
  'model': { phonetic: '/ˈmɒdəl/', partOfSpeech: 'n.', translation: '模型', example: 'a language model' },
  'models': { phonetic: '/ˈmɒdəlz/', partOfSpeech: 'n.', translation: '模型（复数）', example: 'language models' },
  'transform': { phonetic: '/trænsˈfɔːrm/', partOfSpeech: 'v.', translation: '变革，改变，转化', example: 'AI will transform work.' },
  'transforming': { phonetic: '/trænsˈfɔːrmɪŋ/', partOfSpeech: 'v.', translation: '正在变革', example: 'transforming the industry' },
  'natural': { phonetic: '/ˈnætʃərəl/', partOfSpeech: 'adj.', translation: '自然的', example: 'natural language' },
  'processing': { phonetic: '/ˈprɒsesɪŋ/', partOfSpeech: 'n.', translation: '处理，加工', example: 'natural language processing' },
  'architecture': { phonetic: '/ˈɑːrkɪtektʃər/', partOfSpeech: 'n.', translation: '架构，体系结构', example: 'transformer architecture' },
  'training': { phonetic: '/ˈtreɪnɪŋ/', partOfSpeech: 'n.', translation: '训练，培训', example: 'model training' },
  'attention': { phonetic: '/əˈtenʃən/', partOfSpeech: 'n.', translation: '注意力，关注', example: 'self-attention mechanism' },
  'mechanism': { phonetic: '/ˈmekənɪzəm/', partOfSpeech: 'n.', translation: '机制，机理', example: 'attention mechanism' },
  'token': { phonetic: '/ˈtoʊkən/', partOfSpeech: 'n.', translation: '词元，标记', example: 'sequence of tokens' },
  'tokens': { phonetic: '/ˈtoʊkənz/', partOfSpeech: 'n.', translation: '词元（复数）', example: 'billions of tokens' },
  'neural': { phonetic: '/ˈnjʊərəl/', partOfSpeech: 'adj.', translation: '神经的', example: 'neural network' },
  'network': { phonetic: '/ˈnetwɜːrk/', partOfSpeech: 'n.', translation: '网络', example: 'neural network' },
  'gradient': { phonetic: '/ˈɡreɪdiənt/', partOfSpeech: 'n.', translation: '梯度', example: 'gradient descent' },
  'optimization': { phonetic: '/ˌɒptɪmaɪˈzeɪʃən/', partOfSpeech: 'n.', translation: '优化', example: 'optimization algorithm' },
  'parameter': { phonetic: '/pəˈræmɪtər/', partOfSpeech: 'n.', translation: '参数', example: 'model parameters' },
  'parameters': { phonetic: '/pəˈræmɪtərz/', partOfSpeech: 'n.', translation: '参数（复数）', example: 'billions of parameters' },
  'scale': { phonetic: '/skeɪl/', partOfSpeech: 'n./v.', translation: '规模；扩展', example: 'scale of the model' },
  'scaling': { phonetic: '/ˈskeɪlɪŋ/', partOfSpeech: 'n.', translation: '缩放，扩展', example: 'scaling laws' },
  'data': { phonetic: '/ˈdeɪtə/', partOfSpeech: 'n.', translation: '数据', example: 'training data' },
  'performance': { phonetic: '/pərˈfɔːrməns/', partOfSpeech: 'n.', translation: '性能，表现', example: 'model performance' },
  'benchmark': { phonetic: '/ˈbentʃmɑːrk/', partOfSpeech: 'n.', translation: '基准测试，标杆', example: 'NLP benchmark' },
  'benchmarks': { phonetic: '/ˈbentʃmɑːrks/', partOfSpeech: 'n.', translation: '基准测试（复数）', example: 'NLP benchmarks' },
  'application': { phonetic: '/ˌæplɪˈkeɪʃən/', partOfSpeech: 'n.', translation: '应用', example: 'real-world application' },
  'applications': { phonetic: '/ˌæplɪˈkeɪʃənz/', partOfSpeech: 'n.', translation: '应用（复数）', example: 'various applications' },
  'challenge': { phonetic: '/ˈtʃælɪndʒ/', partOfSpeech: 'n.', translation: '挑战', example: 'key challenge' },
  'challenges': { phonetic: '/ˈtʃælɪndʒɪz/', partOfSpeech: 'n.', translation: '挑战（复数）', example: 'significant challenges' },
  'bias': { phonetic: '/ˈbaɪəs/', partOfSpeech: 'n.', translation: '偏见，偏差', example: 'model bias' },
  'biases': { phonetic: '/ˈbaɪəsɪz/', partOfSpeech: 'n.', translation: '偏见（复数）', example: 'harmful biases' },
  'hallucination': { phonetic: '/həˌluːsɪˈneɪʃən/', partOfSpeech: 'n.', translation: '幻觉（AI错误输出）', example: 'LLM hallucination' },
  'inference': { phonetic: '/ˈɪnfərəns/', partOfSpeech: 'n.', translation: '推理，推断', example: 'model inference' },
  'reinforcement': { phonetic: '/ˌriːɪnˈfɔːrsmənt/', partOfSpeech: 'n.', translation: '强化', example: 'reinforcement learning' },
  'learning': { phonetic: '/ˈlɜːrnɪŋ/', partOfSpeech: 'n.', translation: '学习', example: 'machine learning' },
  'feedback': { phonetic: '/ˈfiːdbæk/', partOfSpeech: 'n.', translation: '反馈', example: 'human feedback' },
  'alignment': { phonetic: '/əˈlaɪnmənt/', partOfSpeech: 'n.', translation: '对齐，校准', example: 'model alignment' },
  'safety': { phonetic: '/ˈseɪfti/', partOfSpeech: 'n.', translation: '安全性', example: 'AI safety' },
  'capability': { phonetic: '/ˌkeɪpəˈbɪlɪti/', partOfSpeech: 'n.', translation: '能力，功能', example: 'model capability' },
  'capabilities': { phonetic: '/ˌkeɪpəˈbɪlɪtiz/', partOfSpeech: 'n.', translation: '能力（复数）', example: 'emergent capabilities' },
  'efficiency': { phonetic: '/ɪˈfɪʃənsi/', partOfSpeech: 'n.', translation: '效率', example: 'computational efficiency' },
  'context': { phonetic: '/ˈkɒntekst/', partOfSpeech: 'n.', translation: '上下文，语境', example: 'in-context learning' },
  'sequence': { phonetic: '/ˈsiːkwəns/', partOfSpeech: 'n.', translation: '序列', example: 'token sequence' },
  'vocabulary': { phonetic: '/vəˈkæbjʊleri/', partOfSpeech: 'n.', translation: '词汇表，词汇量', example: 'model vocabulary' },
  'prediction': { phonetic: '/prɪˈdɪkʃən/', partOfSpeech: 'n.', translation: '预测', example: 'next token prediction' },
  'probability': { phonetic: '/ˌprɒbəˈbɪlɪti/', partOfSpeech: 'n.', translation: '概率，可能性', example: 'probability distribution' },
  'distribution': { phonetic: '/ˌdɪstrɪˈbjuːʃən/', partOfSpeech: 'n.', translation: '分布，分配', example: 'probability distribution' },
  'embedding': { phonetic: '/ɪmˈbedɪŋ/', partOfSpeech: 'n.', translation: '嵌入（向量表示）', example: 'word embedding' },
  'generative': { phonetic: '/ˈdʒenərətɪv/', partOfSpeech: 'adj.', translation: '生成的，生成式', example: 'generative model' },
  'emergent': { phonetic: '/ɪˈmɜːrdʒənt/', partOfSpeech: 'adj.', translation: '涌现的，新兴的', example: 'emergent capabilities' },
  'versatile': { phonetic: '/ˈvɜːrsətəl/', partOfSpeech: 'adj.', translation: '多功能的，灵活的', example: 'versatile model' },
  'domain': { phonetic: '/dəˈmeɪn/', partOfSpeech: 'n.', translation: '领域，域', example: 'various domains' },
  'domains': { phonetic: '/dəˈmeɪnz/', partOfSpeech: 'n.', translation: '领域（复数）', example: 'multiple domains' },
  'clinical': { phonetic: '/ˈklɪnɪkəl/', partOfSpeech: 'adj.', translation: '临床的', example: 'clinical settings' },
  'medical': { phonetic: '/ˈmedɪkəl/', partOfSpeech: 'adj.', translation: '医疗的，医学的', example: 'medical diagnosis' },
  'research': { phonetic: '/rɪˈsɜːrtʃ/', partOfSpeech: 'n.', translation: '研究', example: 'AI research' },
  'hypothesis': { phonetic: '/haɪˈpɒθɪsɪs/', partOfSpeech: 'n.', translation: '假设，假说', example: 'research hypothesis' },
  'computational': { phonetic: '/ˌkɒmpjʊˈteɪʃənl/', partOfSpeech: 'adj.', translation: '计算的', example: 'computational cost' },
  'fairness': { phonetic: '/ˈfeərnəs/', partOfSpeech: 'n.', translation: '公平性', example: 'AI fairness' },
  'interpretability': { phonetic: '/ɪnˌtɜːrprɪtəˈbɪlɪti/', partOfSpeech: 'n.', translation: '可解释性', example: 'model interpretability' },
  'evaluation': { phonetic: '/ɪˌvæljuˈeɪʃən/', partOfSpeech: 'n.', translation: '评估，评价', example: 'model evaluation' },
  'framework': { phonetic: '/ˈfreɪmwɜːrk/', partOfSpeech: 'n.', translation: '框架', example: 'evaluation framework' },
  'transformer': { phonetic: '/trænsˈfɔːrmər/', partOfSpeech: 'n.', translation: 'Transformer（神经网络架构）', example: 'transformer model' },
  'paradigm': { phonetic: '/ˈpærədaɪm/', partOfSpeech: 'n.', translation: '范式，范例', example: 'new paradigm' },
  'corpus': { phonetic: '/ˈkɔːrpəs/', partOfSpeech: 'n.', translation: '语料库', example: 'text corpus' },
  'corpora': { phonetic: '/ˈkɔːrpərə/', partOfSpeech: 'n.', translation: '语料库（复数）', example: 'large corpora' },
  'empirical': { phonetic: '/ɪmˈpɪrɪkəl/', partOfSpeech: 'adj.', translation: '实证的，经验的', example: 'empirical study' },
  'residual': { phonetic: '/rɪˈzɪdjuəl/', partOfSpeech: 'adj.', translation: '残差的，剩余的', example: 'residual connection' },
  'normalization': { phonetic: '/ˌnɔːrməlaɪˈzeɪʃən/', partOfSpeech: 'n.', translation: '归一化，标准化', example: 'layer normalization' },
  'downstream': { phonetic: '/ˌdaʊnˈstriːm/', partOfSpeech: 'adj.', translation: '下游的', example: 'downstream task' },
  'fine-tune': { phonetic: '/faɪn tjuːn/', partOfSpeech: 'v.', translation: '微调', example: 'fine-tune the model' },
  'pre-train': { phonetic: '/priː treɪn/', partOfSpeech: 'v.', translation: '预训练', example: 'pre-train on data' },
  'summarization': { phonetic: '/ˌsʌməraɪˈzeɪʃən/', partOfSpeech: 'n.', translation: '摘要，总结', example: 'text summarization' },
  'generation': { phonetic: '/ˌdʒenəˈreɪʃən/', partOfSpeech: 'n.', translation: '生成，产生', example: 'text generation' },
  'substantial': { phonetic: '/səbˈstænʃəl/', partOfSpeech: 'adj.', translation: '巨大的，实质性的', example: 'substantial cost' },
  'profound': { phonetic: '/prəˈfaʊnd/', partOfSpeech: 'adj.', translation: '深远的，深刻的', example: 'profound implications' },
  'equitably': { phonetic: '/ˈekwɪtəbli/', partOfSpeech: 'adv.', translation: '公平地，公正地', example: 'distributed equitably' },
};

export function lookupWord(word: string): WordEntry | null {
  const clean = word.toLowerCase().replace(/[.,!?;:'"()\[\]]/g, '');
  return wordDictionary[clean] || null;
}
